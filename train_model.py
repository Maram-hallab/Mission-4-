# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TxhtOfB9IsyTftLVD4AvO5zSnMQUCjKx
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

# --- STEP 1: LOAD THE DATA ---
file_name = 'datasetpump.csv'

try:
    df = pd.read_csv(file_name)
    print("--- 1. Data Loaded Successfully ---")
    print(df.head())
except FileNotFoundError:
    print(f"--- ERROR ---")
    print(f"File '{file_name}' not found. Did you upload it and rename it correctly?")
except Exception as e:
    print(f"--- ERROR ---")
    print(f"An error occurred loading the file: {e}")
    print("Please check that your file is a proper .csv file.")

# --- STEP 2: PREPARE THE DATA ---
# These are the column names from your file
feature_columns = ['Soil Moisture', 'Temperature', 'Air Humidity']
target_column = 'Pump Data'

# Handle potential data issues
try:
    # Drop rows where any of our needed columns are empty
    df_clean = df.dropna(subset=feature_columns + [target_column])

    # Check if we still have data left
    if df_clean.empty:
        print("\n--- ERROR ---")
        print("After cleaning, no data was left. Please check your file for empty rows/columns.")
    else:
        print(f"\n--- 2. Data Prepared ---")
        print(f"Original rows: {len(df)}, Cleaned rows: {len(df_clean)}")

        # Create the 'X' (features) and 'y' (target) from the *cleaned* data
        X = df_clean[feature_columns]
        y = df_clean[target_column]

        # --- STEP 3: SPLIT THE DATA ---
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        print(f"Total data points: {len(X)}")
        print(f"Training data points: {len(X_train)}")
        print(f"Testing data points: {len(X_test)}")

        # --- STEP 4: TRAIN THE MODEL ---
        print("\n--- 3. Training Model ---")
        # We use RandomForestClassifier
        model = RandomForestClassifier(n_estimators=100, random_state=42)

        # This is the "learning" part
        model.fit(X_train, y_train)
        print("Model training complete!")

        # --- STEP 5: TEST (EVALUATE) THE MODEL ---
        print("\n--- 4. Model Evaluation ---")
        predictions = model.predict(X_test)

        # Check the accuracy
        accuracy = accuracy_score(y_test, predictions)
        print(f"\nModel Accuracy: {accuracy * 100:.2f}%")

        # Get a detailed report
        print("\nClassification Report:")
        print(classification_report(y_test, predictions))

        print("\nConfusion Matrix:")
        print(confusion_matrix(y_test, predictions))

        # --- STEP 6: SAVE THE MODEL ---
        model_filename = 'pump_automation_model.joblib'
        joblib.dump(model, model_filename)
        print(f"\n--- 5. Model Saved! ---")
        print(f"Your trained model is now saved as '{model_filename}'")
        print("You can download it from the 'Files' panel on the left.")

except NameError:
    # This happens if 'df' failed to load
    print("\nStopping script because data did not load correctly.")
except KeyError as e:
    # This happens if the column names are wrong
    print(f"\n--- COLUMN NAME ERROR ---")
    print(f"A column name is wrong: {e}")
    print("Please double-check the 'feature_columns' and 'target_column' variables in this script")
    print("and make sure they EXACTLY match the names in your CSV file.")